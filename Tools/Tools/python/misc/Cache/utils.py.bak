from libimport import *
"""
import sys
import os
import pdb
import csv
import numpy as np
import scipy as sp
from scipy.sparse import *
"""

from p_mat import *
from metrics import *


EXP_DIR = os.environ.get( 'EXP_DIR' )

class Timer:
    def __init__( self ):
        1

    def tic( self ):
        self.start_time = time.time()

    def toc( self ):
        self.stop_time = time.time()
        elapsed_time = self.stop_time - self.start_time
        return elapsed_time

class Logger(object):
    def __init__( self, fil ):
        self.terminal = sys.stdout
        self.log = open( fil, "a" )

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        #this flush method is needed for python 3 compatibility.
        #this handles the flush command by doing nothing.
        #you might want to specify some extra behavior here.
        pass

def myarr( vec ):
    if issparse( vec ):
        return np.asarray( vec.todense() ).flatten()
    else:
        return np.asarray( vec ).flatten()

def myfind( vec ):
    if issparse( vec ):
        vec = vec.todense()
    vec = myarr( vec )
    fd = np.nonzero( vec )[0]
    return fd

def exist_param( param, field ):
    if field in param and param[ field ] is not None:
        return True
    else:
        return False

def read_desc_file( fname ):
    descs = []
    fin = open( fname, 'r' )
    for line in fin:
        descs.append( line )
    fin.close()
    descs = [x.strip() for x in descs]
    return descs

def write_desc_file( X, fname ):
    fout = open( fname, 'w' )
    for item in X:
        print(item, file=fout)
    fout.close()

def read_csv_file( fname, delim=" " ):
    with open( fname ) as csvfile:
        csvreader = csv.reader( csvfile, delimiter=delim )
        rows = [row for row in csvreader]
    rows = np.array( rows ).astype( float )
    return rows

def get_data_dir( dset ):
    global EXP_DIR
    if dset in ['YahooMovie-8K', 'MillionSong-1M', 'MillionSong-100K', 'AIP-400K']:
        data_dir = os.path.join( EXP_DIR, 'Datasets', 'Recommendation', dset )
    elif dset in [ 'DSA-100K', 'DSA-200K', 'new_DSA-200K', 'newest_DSA-200K', 'DSA-2M', 'DSA-7M']:
        data_dir = os.path.join( EXP_DIR, 'Datasets', 'DSA', dset )
    else:
        data_dir = os.path.join( EXP_DIR, 'Datasets', 'MultiLabel', dset )
    return data_dir

def read_split_file( fname ):
    split = read_csv_file( fname )
    split = np.ndarray.flatten( split )
    return split

def load_features_dataset( dset ):
    data_dir = get_data_dir( dset )
    X_Xf_file = os.path.join( data_dir, 'X_Xf' )
    X_Xf = check_read_text_mat( X_Xf_file )
    split = read_split_file( dset )
    trn_X_Xf, tst_X_Xf = split_mat( X_Xf, split )
    return ( trn_X_Xf, tst_X_Xf )

def load_labels_dataset( dset ):
    data_dir = get_data_dir( dset )
    X_Y_file = os.path.join( data_dir, 'X_Y' )
    X_Y = check_read_text_mat( X_Y_file )
    split = read_split_file( dset )
    trn_X_Y, tst_X_Y = split_mat( X_Y, split )
    inv_prop = inv_propensity_wrap( trn_X_Y, dset )
    inv_prop = myarr( inv_prop )
    return ( trn_X_Y, tst_X_Y, inv_prop )

def load_metadata_dataset( dset ):
    data_dir = get_data_dir( dset )
    split = read_split_file( dset )
    X_file = os.path.join( data_dir, 'X.txt' )
    X = check_read_desc_file( X_file )
    Xf_file = os.path.join( data_dir, 'Xf.txt' )
    Xf = check_read_desc_file( Xf_file )
    Y_file = os.path.join( data_dir, 'Y.txt' )
    Y = check_read_desc_file( Y_file )
    Yf_file = os.path.join( data_dir, 'Yf.txt' )
    Yf = check_read_desc_file( Yf_file )
    trn_X = [ X[i] for i in myfind(~split) ]
    tst_X = [ X[i] for i in myfind(split) ]
    return ( trn_X, tst_X, Xf, Y, Yf )    

def load_dataset( dset ):
    data_dir = get_data_dir( dset )
    X_Xf_file = os.path.join( data_dir, 'X_Xf' )
    X_Xf = check_read_text_mat( X_Xf_file )    
    X_Y_file = os.path.join( data_dir, 'X_Y' )
    X_Y = check_read_text_mat( X_Y_file )
    Y_Yf_file = os.path.join( data_dir, 'Y_Yf' )
    Y_Yf = check_read_text_mat( Y_Yf_file )
    X_file = os.path.join( data_dir, 'X.txt' )
    X = check_read_desc_file( X_file )
    Xf_file = os.path.join( data_dir, 'Xf.txt' )
    Xf = check_read_desc_file( Xf_file )
    Y_file = os.path.join( data_dir, 'Y.txt' )
    Y = check_read_desc_file( Y_file )
    Yf_file = os.path.join( data_dir, 'Yf.txt' )
    Yf = check_read_desc_file( Yf_file )
    return ( X_Xf, X_Y, Y_Yf, X, Xf, Y, Yf )

def load_std_xmlc_dataset( dset ):
    trn_X_Xf, tst_X_Xf = load_features_dataset( dset )
    trn_X_Y, tst_X_Y, inv_prop = load_labels_dataset( dset )
    return ( trn_X_Xf, trn_X_Y, tst_X_Xf, tst_X_Y, inv_prop )

def load_std_xmlc_dataset_item_features( dset ):
    data_dir = get_data_dir( dset )
    trn_X_Xf, tst_X_Xf = load_features_dataset( dset )
    trn_X_Y, tst_X_Y, inv_prop = load_labels_dataset( dset )
    Y_Yf_file = os.path.join( data_dir, 'Y_Yf' )
    Y_Yf = check_read_text_mat( Y_Yf_file )
    return ( trn_X_Xf, trn_X_Y, tst_X_Xf, tst_X_Y, inv_prop, Y_Yf )

def print_nonzero_row_names( fout, mat, rows, ind, order="none", k=5 ): #order can be "none", "asc", "desc", "alph"
    assert( ind>=0 and ind<mat.shape[1] )
    triples = []
    for i in range( mat.indptr[ind], mat.indptr[ind+1] ):
        triples.append( ( mat.indices[i], mat.data[i], rows[ mat.indices[i] ] ) )

    if order=="asc":
        triples = sorted( triples, key=lambda tup: tup[1] )
    elif order=="desc":
        triples = sorted( triples, key=lambda tup: -tup[1] )
    elif order=="alph":
        triples = sorted( triples, key=lambda tup: tup[2] )
    else:
        triples = sorted( triples, key=lambda tup: tup[0] )

    ctr = 0
    for trip in triples:
        if ctr<k:
            print("%d\t%f\t%s" %( trip[0], trip[1], trip[2] ), file=fout)
            ctr += 1

def print_prediction_mat( out_file, score_mat, tst_X_Y, X, Y, per_point_metrics=None, sampleno=0, samplesize=1000, transpose=False ):
    if transpose:
        score_mat = transpose_mat( score_mat )
        tst_X_Y = transpose_mat( tst_X_Y )
        tmp = X
        X = Y
        Y = tmp

    fout = open( out_file, 'w' )

    num_X = tst_X_Y.shape[1]
    num_Y = tst_X_Y.shape[0]
    rd.seed( sampleno )
    inds = list(range( num_X))
    rd.shuffle( inds )

    for i in range( samplesize ):
        ind = inds[ i ]
        print(X[ ind ], file=fout)
        print("", file=fout)
        print_nonzero_row_names( fout, tst_X_Y, Y, ind, order="desc", k=100 )
        print("", file=fout)
        if per_point_metrics is not None:
            print(per_point_metrics[ ind ], file=fout)
        print_nonzero_row_names( fout, score_mat, Y, ind, order="desc", k=50 )
        print("\n", file=fout)
    fout.close()


def get_random_split( N, frac, seed=0 ):
    rd.seed( seed )
    seq = list(range( N))
    test = rd.sample( seq, int(N*frac) )
    vec = np.zeros( N )
    vec[ test ] = 1
    vec = np.array( vec>0 )
    return vec

def split_vec( vec, split ):
    testvec = [ vec[i] for i in myfind( split ) ]
    trainvec = [ vec[i] for i in myfind( ~split ) ]
    return (trainvec, testvec)

def split_dataset( X_Xf, X_Y, X, frac ):
    num_inst = X_Xf.shape[1]
    split = get_random_split( num_inst, frac )
    ( trn_X_Xf, tst_X_Xf ) = split_mat( X_Xf, split )
    ( trn_X_Y, tst_X_Y ) = split_mat( X_Y, split )
    ( trn_X, tst_X ) = split_vec( X, split )
    return ( trn_X_Xf, tst_X_Xf, trn_X_Y, tst_X_Y, trn_X, tst_X ) 
